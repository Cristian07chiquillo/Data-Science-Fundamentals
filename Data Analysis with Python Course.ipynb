{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8befa6e7",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1> Fundamentos en Ciencia de Datos </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a924d",
   "metadata": {},
   "source": [
    "### Metodología de Trabajo de John Rollins\n",
    "\n",
    "John Rollins propuso una metodología estructurada para el trabajo en ciencia de datos, que consta de varias etapas clave para abordar de manera efectiva los problemas de análisis de datos y modelado predictivo.\n",
    "\n",
    "1. **Definir el Problema**: Comprender claramente el problema o la pregunta de investigación que se abordará.\n",
    "2. **Recopilación de Datos**: Recolectar los datos relevantes necesarios para el análisis.\n",
    "3. **Exploración de Datos**: Realizar un análisis exploratorio de los datos para comprender mejor su naturaleza y relaciones.\n",
    "4. **Preprocesamiento de Datos**: Limpiar, transformar y preparar los datos para el modelado.\n",
    "5. **Modelado**: Desarrollar modelos predictivos utilizando técnicas adecuadas según el problema.\n",
    "6. **Evaluación**: Evaluar el rendimiento y la validez de los modelos construidos.\n",
    "7. **Despliegue**: Implementar soluciones basadas en los resultados del análisis y modelado.\n",
    "\n",
    "A continuación, se presenta un esquema visual de la metodología propuesta por John Rollins:\n",
    "\n",
    "![Metodología de Trabajo de John Rollins](https://www.researchgate.net/publication/340292606/figure/fig14/AS:963472573595670@1606721170929/Foundational-methodology-for-data-science-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c40dee9",
   "metadata": {},
   "source": [
    "# 1. Recolección de Datos\n",
    "En esta sección, se obtienen los datos automotrices desde una URL utilizando la librería Pandas. Los datos se almacenan en un DataFrame para su posterior análisis y modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d9976d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL del conjunto de datos\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\"\n",
    "\n",
    "# Cargar datos desde la URL\n",
    "df = pd.read_csv(url, header = None)\n",
    "\n",
    "# Definir los nombres de las columnas\n",
    "headers = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n",
    "         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n",
    "         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n",
    "         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n",
    "df.columns = headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41f657",
   "metadata": {},
   "source": [
    "## 2. Comprensión de los Datos\n",
    "Se realizan análisis exploratorios para comprender la estructura de los datos. Esto incluye visualizar las primeras y últimas filas del DataFrame, verificar los tipos de datos y generar estadísticas descriptivas básicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "361a6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar las primeras y últimas filas\n",
    "df.head(5)\n",
    "df.tail(5)\n",
    "\n",
    "# Verificar tipos de datos\n",
    "df.dtypes\n",
    "\n",
    "# Resumen estadístico\n",
    "df.describe()\n",
    "df.describe(include=\"all\")\n",
    "\n",
    "# Información concisa del dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85af78",
   "metadata": {},
   "source": [
    "## 3. Preparación de los Datos\n",
    "En esta etapa, se preparan los datos para su procesamiento y modelado. Esto implica manejar valores faltantes, convertir tipos de datos y realizar transformaciones como normalización y binning.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://codecamp.ru/content/images/2022/06/numpyaxis1.jpg\" width=\"700\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea9094c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Identificar y manejar valores faltantes\n",
    "def identify_values(dataframe):\n",
    "    missing_data = dataframe.isnull()\n",
    "\n",
    "    # Contar valores faltantes por columna\n",
    "    for column in missing_data.columns.values.tolist():\n",
    "        print(column)\n",
    "        print (missing_data[column].value_counts())\n",
    "        print(\"\")\n",
    "    \n",
    "# Eliminar filas/columnas con valores faltantes \n",
    "df.dropna(inplace = True)\n",
    "\n",
    "# Eliminar la fila completa\n",
    "df.dropna(axis = 0, inplace = True)\n",
    "\n",
    "# Eliminar la columna completa\n",
    "df.dropna(axis = 1, inplace = True)\n",
    "\n",
    "# Eliminar valores faltantes en una columna dada.\n",
    "df = df.dropna(subset=[\"price\"], axis=0)\n",
    "df = df.dropna(subset=[\"price\"], axis=0, inplace = True)\n",
    "\n",
    "# Reemplazar valores faltantes\n",
    "df.replace('?',np.NaN)\n",
    "df.replace('?',np.NaN, inplace = True)\n",
    "\n",
    "# Reemplazar valores faltantes con la media\n",
    "columns_names = ['normalized-losses','stroke','bore','horsepower','peak-rpm']\n",
    "for column in columns_names:\n",
    "    # Calcular la media\n",
    "    mean = df[column].astype('float').mean()\n",
    "    \n",
    "    # Remplazar la media en los valores faltantes\n",
    "    df[column].replace(np.nan,mean, inplace = True)\n",
    "\n",
    "# Reemplazar valores faltantes con el valor más frecuente\n",
    "df['num-of-doors'].replace(np.nan,'four',inplace = True)\n",
    "\n",
    "# Corregir las unidades de las columnas\n",
    "df['city-mpg'] = 235/df['city-mpg']\n",
    "df.rename(columns = {\"city-mpg\":\"city-L/100km)\"},inplace = True)\n",
    "\n",
    "# Corregir el tipo de dato\n",
    "df['price'] = df['price'].astype(\"int\")\n",
    "df[[\"bore\", \"stroke\"]] = df[[\"bore\", \"stroke\"]].astype(\"float\")\n",
    "\n",
    "# Finalmente, se reinicia el index debido a que se eliminaron columnas\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Formas de obtener los diferentes valores \n",
    "Min_value = df['price'].min()\n",
    "Max_value = df['price'].max()\n",
    "Mean_value = df['price'].mean()\n",
    "Std_value = df['price'].std()\n",
    "\n",
    "# Normalización de los datos\n",
    "df['height'] = df['height']/df['height'].max()\n",
    "\n",
    "# Convertir valores númericos en valores categóricos\n",
    "bins = np.linspace(min(df['price']), max(df['price']), 4) # Coloca un número de más de las categorías que deseas tener\n",
    "group_names = ['Low', 'Medium','High']\n",
    "df['price_categories'] = pd.cut(df['price'], bins, labels = group_names, include_lowest = True)\n",
    "\n",
    "# Convertir valores categóricos en valores númericos\n",
    "dummy_variable = pd.get_dummies(df['fuel-type'])\n",
    "dummy_variable.rename(columns={'gas':'fuel-type-gas', 'diesel':'fuel-type-diesel'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e92aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A este punto, es importante guardar el dataframe para su uso en la etapa de modelado\n",
    "\n",
    "# Guardar el dataframe en un archivo CSV\n",
    "df.to_csv('df_clean.csv', index=False)\n",
    "\n",
    "# Guardar el dataframe en un archivo Excel\n",
    "df.to_excel('df_clean.xlsx', index=False)\n",
    "\n",
    "# Guardar el dataframe en un archivo JSON\n",
    "df.to_json('df_clean.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6152522",
   "metadata": {},
   "source": [
    "## 4. Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80e372",
   "metadata": {},
   "source": [
    "### 4.1. Análisis estadístico y gráfico\n",
    "\n",
    "En esta sección, realizamos análisis estadísticos y modelado predictivo según sea necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead65e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma antes del Binning\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as plt\n",
    "from matplotlib import pyplot\n",
    "plt.pyplot.hist(df[\"price\"])\n",
    "\n",
    "# Establecer título a los ejes y al grafico\n",
    "plt.pyplot.xlabel(\"price\")\n",
    "plt.pyplot.ylabel(\"count\")\n",
    "plt.pyplot.title(\"Before Binning Process\")\n",
    "\n",
    "# Histograma después del Binning\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as plt\n",
    "from matplotlib import pyplot\n",
    "plt.pyplot.hist(df[\"price_categories\"])\n",
    "\n",
    "# Establecer etiquetas y título\n",
    "plt.pyplot.xlabel(\"price_categories\")\n",
    "plt.pyplot.ylabel(\"count\")\n",
    "plt.pyplot.title(\"After Binning Process\")\n",
    "\n",
    "# Resumen estadístico\n",
    "df.describe()\n",
    "df.describe(include = \"all\")\n",
    "\n",
    "# Resumir datos categóricos\n",
    "drive_wheels_count = df['drive_wheels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df24002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots \n",
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=\"drive-wheels\", y=\"price\", data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feed4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear el Scatter Plot\n",
    "X = df[\"engine-size\"]\n",
    "Y = df[\"price\"]\n",
    "plt.scatter(X, Y)\n",
    "\n",
    "# Configurar etiquetas y título\n",
    "plt.xlabel(\"Engine Size\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Scatter Plot of Engine Size vs Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un nuevo DataFrame\n",
    "df_test = df[['drive-wheels','body-style','price']]\n",
    "\n",
    "# Agrupar por 'drive-wheels' y 'body-style' y calcular la media del precio\n",
    "df_grp = df_test.groupby(['drive-wheels','body-style'], as_index = False).mean()\n",
    "df_grp\n",
    "\n",
    "# Crear una tabla pivot\n",
    "df_pivot = df_grp.pivot(index = 'drive-wheels',columns = 'body-style', values = 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366cdd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Crear el heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_pivot, annot=True, cmap='hot', fmt='.0f')\n",
    "\n",
    "# Configurar etiquetas y título\n",
    "plt.xlabel('Body Style')\n",
    "plt.ylabel('Drive Wheels')\n",
    "plt.title('Heatmap of Car Prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Plot\n",
    "import seaborn as sns\n",
    "sns.regplot(x=\"engine-size\", y = \"price\", data = df)\n",
    "plt.ylim(0,)\n",
    "\n",
    "# Configurar etiquetas y título\n",
    "plt.xlabel('Engine Size')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Correlation between Price and Engine Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57fd2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation usando un Heatmap\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular la correlación de Pearson\n",
    "pearson_coef, p_value = pearsonr(df['horsepower'], df['price'])\n",
    "\n",
    "# Seleccionar solo columnas numéricas\n",
    "df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "corr_matrix = df_numeric.corr()\n",
    "\n",
    "# Crear el heatmap usando Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# Configurar el título\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "\n",
    "# Mostrar el heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Seleccionar columnas relevantes para el análisis\n",
    "df_anova = df[['make', 'price']]\n",
    "\n",
    "# ANOVA entre Honda y Subaru\n",
    "anova_results = f_oneway(df_anova[df_anova['make'] == 'honda']['price'], df_anova[df_anova['make'] == 'subaru']['price'])\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Valor F1:\", anova_results.statistic)\n",
    "print(\"Valor p:\", anova_results.pvalue)\n",
    "\n",
    "if anova_results.pvalue < 0.05:\n",
    "    print(\"Hay diferencias significativas entre los grupos (Honda y Subaru).\")\n",
    "else:\n",
    "    print(\"No hay diferencias significativas entre los grupos (Honda y Subaru).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb9022",
   "metadata": {},
   "source": [
    "### 4.2. Modelos de Regresión\n",
    "En la fase de modelado, se aplican técnicas de regresión para predecir el precio de los automóviles en base a diferentes características. A continuación se presentan algunos modelos utilizados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e11230",
   "metadata": {},
   "source": [
    "####  4.2.1. Regresión Lineal (LR)\n",
    "Este tipo de regresión es conocida por su ecuación:\n",
    "\n",
    "$$\n",
    "Yhat = a + b  X\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión Lineal\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df['hightway-mpg'] # Variable independiente\n",
    "Y = df['price'] # Variable dependiente\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3,random_state = 0)\n",
    "\n",
    "# Crear el modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# Obtener los coeficientes del modelo\n",
    "model.intercept_\n",
    "model.coef_\n",
    "\n",
    "# Crear predicciones\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c179a4",
   "metadata": {},
   "source": [
    "#### 4.2.2. Regresión Polinomial (PR)\n",
    "\n",
    "Este tipo de regresión se utiliza para describir relaciones curvilíneas. En este tipo de regresión puedes encontrar diferentes modelos como:\n",
    "\n",
    "<center><b>Quadrática - Orden 2</b></center>\n",
    "\n",
    "$$\n",
    "\\hat{Y} = a + b_1X + b_2X^2\n",
    "$$\n",
    "\n",
    "<center><b>Cúbica - Orden 3</b></center>\n",
    "\n",
    "$$\n",
    "\\hat{Y} = a + b_1X + b_2X^2 + b_3X^3\n",
    "$$\n",
    "\n",
    "<center><b>De Orden Superior</b></center>\n",
    "\n",
    "$$\n",
    "Y = a + b_1X + b_2X^2 + b_3X^3 + \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión Polinomial\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Split data between train and test\n",
    "Z = df[['horsepower','curb-weight','engine-size','highway-L/100km']]\n",
    "Y = df['price']\n",
    "Z_train, Z_test, Y_train, Y_test = train_test_split(Z, Y, test_size = 0.3,random_state = 0)\n",
    "\n",
    "# Normalize data \n",
    "Z_train_scaled = SCALE.fit_transform(Z_train)  # Normalize training data\n",
    "Z_test_scaled = SCALE.transform(Z_test)  # Normalize test data\n",
    "\n",
    "# Calculate Polynomial (3rd order, in this case)\n",
    "pr = PolynomialFeatures(degree=3, include_bias=False)\n",
    "Z_polly_train = pr.fit_transform(Z_train_scaled) # Apply polynomial transformation to normalized training data\n",
    "Z_polly_test = pr.transform(Z_test_scaled) # Apply polynomial transformation to normalized test data\n",
    "\n",
    "# Fit the regression model to the training data\n",
    "model = LinearRegression()\n",
    "model.fit(Z_polly_train, Y_train)\n",
    "\n",
    "# Generate predictions with the test data\n",
    "Y_pred = model.predict(Z_polly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste de un polinomio de 11º orden (cúbico)\n",
    "\n",
    "f1 = np.polyfit(X, Y, 11)\n",
    "p1 = np.poly1d(f1)\n",
    "\n",
    "# Función para graficar el polinomio\n",
    "def PlotPolly(model, independent_variable, dependent_variabble, Name):\n",
    "    # Generar nuevos puntos para la curva ajustada\n",
    "    X_new = np.linspace(15, 55, 100)\n",
    "    Y_new = model(X_new)\n",
    "    \n",
    "    # Graficar los datos originales y la curva ajustada\n",
    "    plt.plot(independent_variable, dependent_variabble, '.', X_new, Y_new, '-')\n",
    "    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor((0.898, 0.898, 0.898)) # Color de fondo del gráfico\n",
    "    fig = plt.gcf()\n",
    "    plt.xlabel(Name)\n",
    "    plt.ylabel('Price of Cars')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Llamar a la función para graficar el polinomio\n",
    "PlotPolly(p1,x,y, 'Highway MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9140f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]\n",
    "Y = df['price']\n",
    "Z_train, Z_test, Y_train, Y_test = train_test_split(Z, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Crear el pipeline\n",
    "Input = [('polynomial', PolynomialFeatures(degree=2)), ('scale', StandardScaler()), ('model', LinearRegression())]\n",
    "Pipe = Pipeline(Input)\n",
    "\n",
    "# Entrenar el pipeline (ajustar el modelo)\n",
    "Pipe.fit(Z_train, Y_train)\n",
    "\n",
    "# Generar predicciones para los datos de prueba \n",
    "Y_pred = Pipe.predict(Z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca3bc8",
   "metadata": {},
   "source": [
    "#### 4.2.3. Regresión Multiple (MLR)\n",
    "Este tipo de regresión es conocida por su ecuación:\n",
    "\n",
    "$$\n",
    "Yhat = a + b\\_1 X\\_1 + b\\_2 X\\_2 + b\\_3 X\\_3 + b\\_4 X\\_4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión Multiple\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "Z = df[['horsepower','curb-weight','engine-size','hightway-mpg']]\n",
    "Y = df['price']\n",
    "Z_train, Z_test, Y_train, Y_test = train_test_split(Z, Y, test_size = 0.3,random_state = 0)\n",
    "\n",
    "# Crear el modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(Z_train,Y_train)\n",
    "\n",
    "# Obtener los coeficientes del modelo\n",
    "model.intercept_\n",
    "model.coef_\n",
    "\n",
    "# Crear predicciones\n",
    "Y_pred = model.predict(Z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37065d1",
   "metadata": {},
   "source": [
    "#### 4.2.4. Regresión Ridge\n",
    "La regularización en la regresión Ridge es controlada por un parámetro de regularización (α), que controla la fuerza de la penalización aplicada a los coeficientes. Un valor más alto de (α) conduce a coeficientes más pequeños y a un modelo más regularizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha_value = [0,0.01, 0.1, 1, 10] # Valores mayores de alpha producen subajuste\n",
    "R_squared_test = []\n",
    "\n",
    "for value in alpha_value:\n",
    "    # Dividir los datos entre entrenamiento y prueba\n",
    "    Z = df[['horsepower','curb-weight','engine-size','highway-L/100km']]\n",
    "    Y = df['price']\n",
    "    Z_train, Z_test, Y_train, Y_test = train_test_split(Z, Y, test_size = 0.3,random_state = 0)\n",
    "\n",
    "    # Crear el modelo Ridge\n",
    "    RidgeModel = Ridge(alpha = value)\n",
    "\n",
    "    # Ajustar el modelo de regresión a los datos de entrenamiento\n",
    "    RidgeModel.fit(Z_train,Y_train)\n",
    "\n",
    "    # Generar predicciones con el modelo \n",
    "    Y_pred = RidgeModel.predict(Z_test)\n",
    "    \n",
    "    # Evaluar R^2 con los datos de prueba\n",
    "    R_squared_test.append(RidgeModel.score(Z_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d704ceb",
   "metadata": {},
   "source": [
    "## 5. Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7b917",
   "metadata": {},
   "source": [
    "En esta sección, evaluamos el rendimiento del modelo predictivo desarrollado en la fase de modelado. Utilizamos métricas específicas para medir la precisión, la robustez y la generalización del modelo frente a datos nuevos. Además, realizamos análisis detallados de errores y validación cruzada para garantizar la fiabilidad de las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f320f8",
   "metadata": {},
   "source": [
    "### 5.1.  Evaluación Gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b901ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Plot\n",
    "\n",
    "import seaborn as sns\n",
    "sns.regplot(X=\"engine-size\", Y = \"price\", data = df)\n",
    "plt.ylim(0,) # Limitar el eje y para comenzar en 0\n",
    "\n",
    "# Configurar etiquetas y título\n",
    "plt.xlabel('Engine Size')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Correlation between Price and Engine Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50956baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Plot\n",
    "\n",
    "import seaborn as sns\n",
    "sns.residplot(df['hightway-mpg'], df['price']) \n",
    "plt.show() # Mostrar el gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Plot\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Crear el gráfico de distribución\n",
    "ax1 = sns.distplot(df['price'],hist = False, color = 'r', label = 'Actual Value')\n",
    "sns.distplot(Y_pred,hist = False, color = 'b', label = 'Fitted Values', ax = ax1)\n",
    "\n",
    "# Configurar título y etiquetas\n",
    "plt.title('Actual vs Fitted Values for Price')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Proportion of laptops')\n",
    "plt.legend(['Actual Value', 'Predicted Value'])\n",
    "plt.show()  # Mostrar el gráfico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede91bca",
   "metadata": {},
   "source": [
    "<h3> 5.2. Desempeño del Modelo </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Cuadrático Medio (MSE)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calcular el MSE\n",
    "mse_slr = mean_squared_error(Y_test,Y_pred)\n",
    "print('The MSE for your model is: ', mse_slr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-Cuadrado (R²)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calcular R²\n",
    "r2_test = r2_score(Y_test, Y_pred)\n",
    "print(\"R-squared (R²) - Test data:\", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256712ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Calcular los puntajes de validación cruzada para el modelo\n",
    "scores = cross_val_score(model, X_train, Y_train, cv = 3)\n",
    "\n",
    "# Calcular el promedio de los puntajes\n",
    "mean_score = np.mean(scores)\n",
    "\n",
    "# Imprimir el promedio de los puntajes\n",
    "print(\"Promedio de puntajes de validación cruzada:\", mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90068526",
   "metadata": {},
   "source": [
    "<h3> 5.3. Comparación de Modelos </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad378434",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [1,2,3,4,5]\n",
    "R_squared_test = []\n",
    "\n",
    "# Dividir los datos entre entrenamiento y prueba\n",
    "Z = df[['horsepower','curb-weight','engine-size','highway-L/100km']]\n",
    "Y = df['price']\n",
    "Z_train, Z_test, Y_train, Y_test = train_test_split(Z, Y, test_size = 0.3,random_state = 0)\n",
    "\n",
    "for n in order:\n",
    "    # Calcular la transformación polinomial\n",
    "    pr = PolynomialFeatures(degree = n)\n",
    "    Z_polly_train = pr.fit_transform(Z_train) # Aplicar transformación polinomial a los datos de entrenamiento\n",
    "    Z_polly_test = pr.transform(Z_test) # Aplicar transformación polinomial a los datos de prueba\n",
    "\n",
    "    # Ajustar el modelo de regresión a los datos de entrenamiento\n",
    "    model = LinearRegression()\n",
    "    model.fit(Z_polly_train, Y_train)\n",
    "\n",
    "    # Evaluar R^2 con los datos de prueba\n",
    "    R_squared_test.append(model.score(Z_polly_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e4fbc",
   "metadata": {},
   "source": [
    "<h3> 5.4. Comparación de Hiperparámetros </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d59276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a list of parameters\n",
    "parameters = [{'alpha':[0.001,0.01,0.1,1,10], 'normalize':[True,False]}]\n",
    "\n",
    "# Create a Ridge Model\n",
    "RidgeModel = Ridge()\n",
    "\n",
    "# Create Grid\n",
    "Grid = GridSearchCV(RidgeModel,parameters,cv=4)\n",
    "\n",
    "# Fit Grid\n",
    "Grid.fit(X,Y)\n",
    "\n",
    "# Select best combination of parameters\n",
    "Grid.best_estimator_\n",
    "\n",
    "# Store results\n",
    "scores = Grid.cv_results_\n",
    "scores ['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4623b50",
   "metadata": {},
   "source": [
    "##  6. Conceptos Técnicos\n",
    "\n",
    "\n",
    "### 6.1. Preprocesamiento de Datos\n",
    "- **Binning**: Transforma variables numéricas en categorías discretas para simplificar modelos y manejar valores atípicos.\n",
    "- **Normalización**:\n",
    "  - **Min-Max Scaling**: Escala los valores al rango [0, 1].\n",
    "  \n",
    "$$ \\text{New_value} = \\frac{\\text{Old_value} - \\text{Min_value}}{\\text{Max_value} - \\text{Min_value}} $$\n",
    " \n",
    "  - **Z-score Standardization**: Transforma valores para tener media 0 y desviación estándar 1.\n",
    "  \n",
    "$$ \\text{New_value} = \\frac{\\text{Old_value} - \\text{Avg_value}}{\\text{Std_value}} $$\n",
    "\n",
    "  - **Robust Scaling**: Utiliza la mediana y rango intercuartílico, robusto ante valores atípicos.\n",
    "  - **Log Transformation**: Aplica logaritmo para reducir varianza y mejorar distribución de datos.\n",
    "  \n",
    "### 6.2. Modelado Predictivo\n",
    "- **Modelo de Regresión Lineal**: Establece relación lineal entre variables.\n",
    "- **Modelo de Regresión Polinomial**: Captura relaciones no lineales mediante polinomios.\n",
    "- **Regresión Ridge**: Controla sobreajuste con penalización en función de coste.\n",
    "\n",
    "### 6.3. Evaluación de Modelos\n",
    "- **Underfitting y Overfitting**:\n",
    "  - **Underfitting**: Modelo demasiado simple que no captura la complejidad de los datos.\n",
    "  - **Overfitting**: Modelo demasiado complejo que se ajusta demasiado a los datos de entrenamiento.\n",
    "  \n",
    "<center>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1200/1*YQ5tjb1TqNHenYMFk2tPog.png\" width=\"700\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n",
    "\n",
    "- **MSE (Error Cuadrático Medio)**: Mide la discrepancia entre valores reales y predichos por el modelo.\n",
    "- **R-Cuadrado (R²)**: Indica la proporción de varianza explicada por el modelo.\n",
    "- **Validación Cruzada (Cross-Validation)**: Evalúa el rendimiento del modelo con múltiples divisiones de datos.\n",
    "\n",
    "### 6.4. Análisis Estadístico\n",
    "\n",
    "#### ANOVA (Análisis de Varianza)\n",
    "- **Propósito**: Compara medias de grupos para identificar diferencias significativas.\n",
    "\n",
    "#### Coeficiente F (F-statistic)\n",
    "- **Significado**: Indica la relación entre la varianza explicada por el modelo y la varianza no explicada.\n",
    "- **Interpretación**: Un valor alto de F indica que la varianza entre los grupos es mayor que la varianza dentro de los grupos, lo que sugiere que al menos un grupo tiene una media diferente.\n",
    "\n",
    "#### Valor p (p-value)\n",
    "- **Significado**: Indica la significancia estadística del coeficiente F.\n",
    "- **Interpretación**:\n",
    "  - **p-value < 0.05**: Existe evidencia significativa para rechazar la hipótesis nula, lo que sugiere que al menos un grupo tiene una media diferente.\n",
    "  - **p-value > 0.05**: No hay evidencia suficiente para rechazar la hipótesis nula, lo que sugiere que no hay diferencias significativas entre las medias de los grupos.\n",
    "\n",
    "### 6.5. Técnicas Avanzadas\n",
    "- **Pipeline**: Encadena procesos como normalización y modelado para automatizar flujos de trabajo.\n",
    "- **Transformación Polinomial**: Añade términos polinomiales para capturar relaciones no lineales.\n",
    "- **GridSearchCV**: Busca mejores hiperparámetros para un modelo mediante búsqueda exhaustiva."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
